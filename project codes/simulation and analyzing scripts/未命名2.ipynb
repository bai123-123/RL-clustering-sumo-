{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "\n",
    "import paddle.fluid as fluid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MujocoModel(parl.Model):\n",
    "\n",
    "    #set act dim = 3\n",
    "    #set obs dim = 25 * 3\n",
    "\n",
    "    def __init__(self, act_dim):\n",
    "        self.actor_model = ActorModel(act_dim)\n",
    "        self.critic_model = CriticModel()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()\n",
    "\n",
    "\n",
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        hid1_size = 400\n",
    "        hid2_size = 300\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=act_dim, act='tanh')\n",
    "\n",
    "    def policy(self, obs):\n",
    "        hid1 = self.fc1(obs)\n",
    "        hid2 = self.fc2(hid1)\n",
    "        means = self.fc3(hid2)\n",
    "        return means\n",
    "\n",
    "\n",
    "class CriticModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        hid1_size = 400\n",
    "        hid2_size = 300\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        hid1 = self.fc1(obs)\n",
    "        concat = layers.concat([hid1, act], axis=1)\n",
    "        hid2 = self.fc2(concat)\n",
    "        Q = self.fc3(hid2)\n",
    "        Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q\n",
    "    \n",
    "    \n",
    "    \n",
    "class MujocoAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(MujocoAgent, self).__init__(algorithm)\n",
    "\n",
    "        # Attention: In the beginning, sync target model totally.\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        act = np.squeeze(act)\n",
    "        act = np.absolute(act)\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "        return critic_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calDis(p1, p2):\n",
    "\n",
    "     dx = float(p1[0]) - float(p2[0])\n",
    "     dy = float(p1[1]) - float(p2[1])\n",
    "     return math.sqrt((dx**2)+(dy**2))\n",
    "\n",
    "\n",
    "def calSpeedD(v1, v2):\n",
    "     v1 = float(v1)\n",
    "     v2 = float(v2)\n",
    "     return math.fabs(v1 - v2)\n",
    "\n",
    "\n",
    "def calAccD(a1, a2):\n",
    "     a1 = float(a1)\n",
    "     a2 = float(a2)\n",
    "     return math.fabs(a1 - a2)\n",
    "\n",
    "\n",
    "def getDifferenceData(obs):\n",
    "\n",
    "     diffList = []\n",
    "     totalDiffPos = 0\n",
    "     totalDifSpeed = 0\n",
    "     totalDifAcc = 0\n",
    "     communicationRange = 30\n",
    "     averageSpeed = 0\n",
    "     averageAcc = 0\n",
    "     connection ={}\n",
    "     for v in obs:\n",
    "\n",
    "          vCollectedDis = []\n",
    "          vCollectedSpeedDis = []\n",
    "          vCollectedAccDis = []\n",
    "          vCollectCommCars = []\n",
    "          averageSpeed += v[2]\n",
    "          averageAcc += v[3]\n",
    "          \n",
    "          for i in range(0, len(obs)):\n",
    "               tempCar = obs[i]\n",
    "               if v[0] != tempCar[0]:\n",
    "                    distance = calDis(v[1], tempCar[1])   \n",
    "                    if distance < communicationRange:\n",
    "                        vCollectCommCars.append(tempCar[0])\n",
    "                        vCollectedDis.append(distance)\n",
    "                        speedDis = calSpeedD(v[2], tempCar[2])\n",
    "                        vCollectedSpeedDis.append(speedDis)\n",
    "                        accDis = calAccD(v[3], tempCar[3])\n",
    "                        vCollectedAccDis.append(accDis)\n",
    "                    \n",
    "\n",
    "          diffList.append([v[0], np.mean(vCollectedDis), np.mean(\n",
    "          vCollectedSpeedDis), np.mean(vCollectedAccDis), vCollectCommCars])\n",
    "          connection[v[0]] = vCollectCommCars\n",
    "     vehicleNum = len(diffList)\n",
    "     obsMobility = [0,0,0]\n",
    "     if vehicleNum !=0:  \n",
    "         obsMobility = [averageSpeed / vehicleNum, averageSpeed / vehicleNum ]\n",
    "     result = [obsMobility ,connection ]\n",
    "     # print(len( diffList))\n",
    "     return diffList, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterCreation(diffList, clusterNumber, obsPD, obs, obsMobility,action ):\n",
    "#      w1 = 0.2\n",
    "#      w2 = 0.4\n",
    "#      w3 = 0.4\n",
    "#      w1 = 0.6\n",
    "#      w2 = 0.1\n",
    "#      w3 = 0.3\n",
    "#      w1 = 1\n",
    "#      w2 = 1\n",
    "#      w3 = 1\n",
    "     w1  = action[0]\n",
    "     w2 = action[1]\n",
    "     w3 = action[2]\n",
    "     t = 0\n",
    "     dic = {}\n",
    "#      obsMobility = obsMobility[0]\n",
    "     for singleV in diffList:\n",
    "            if len(singleV[4]) not in dic:\n",
    "                dic[len(singleV[4])]  = [singleV[0]]\n",
    "            else:\n",
    "                dic[len(singleV[4])].append(singleV[0])\n",
    "     if len(dic.keys()) > 3:\n",
    "       \n",
    "        clusters = sorted(dic.keys(),reverse=True)[:3]\n",
    "        \n",
    "     else:\n",
    "        clusters = sorted(dic.keys(),reverse=True)\n",
    "    \n",
    "#      for x in dic:\n",
    "#             print(x)\n",
    "#      print(\"zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "    \n",
    "#      for i in action:\n",
    "#         t += abs(i)\n",
    "#      w1 = action[0] / t\n",
    "#      w2 = action[1] /t\n",
    "#      w3 = action[2] /t\n",
    "     tempScore = 100000\n",
    "     communicationRange = 30\n",
    "     sequenceScores = []\n",
    "     if obsMobility[0] == 0:\n",
    "        obsMobility[0] = 1\n",
    "     if obsMobility[1] == 0:\n",
    "        obsMobility[1] = 1\n",
    "    \n",
    "     scoreDic = {}\n",
    "     for singleV in diffList:\n",
    "          score = w1 * singleV[1] / 30  + w2 * singleV[2] / obsMobility[0] + w3 * singleV[3] / obsMobility[1]\n",
    "          scoreDic[singleV[0]] = score\n",
    "        \n",
    "     clusterHeads = {}\n",
    "     for c in clusters:\n",
    "        s =  1000\n",
    "        \n",
    "        if len(dic[c]) > 1:\n",
    "            clusterHead = \"\"\n",
    "            for v in dic[c]:\n",
    "                if scoreDic[v] < s:\n",
    "                    clusterHead = v\n",
    "                    s = scoreDic[v]\n",
    "                \n",
    "#         print(clusterHead)\n",
    "            if clusterHead !='':\n",
    "                clusterHeads[clusterHead] = c\n",
    "     \n",
    "     return clusterHeads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentClusters = []\n",
    "def highway(action):\n",
    "    clusterChange = False\n",
    "    obsMobility = 0 \n",
    "    reward = 0\n",
    "    obsOut = []\n",
    "    done = False\n",
    "    while clusterChange != True:\n",
    "        ls = traci.vehicle.getIDList()\n",
    "        obs = []\n",
    "        for veh_id in ls:\n",
    "            position = traci.vehicle.getPosition(veh_id)\n",
    "            speed = traci.vehicle.getSpeed(veh_id)\n",
    "            acceleration = traci.vehicle.getAcceleration(veh_id)\n",
    "            singleV = [veh_id, position, speed, acceleration]\n",
    "            obs.append(singleV)\n",
    "                        #   print(veh_id ,position,speed,acceleration)\n",
    "\n",
    "\n",
    "\n",
    "              # print(obs,reward)\n",
    "        obsPD = pd.DataFrame(\n",
    "              obs, columns=['V-ID', 'Position', 'Speed', 'Acc'])\n",
    "        \n",
    "        difflist, obsMobility = getDifferenceData(obs)\n",
    "        if traci.simulation.getMinExpectedNumber() == 3 :\n",
    "            done = True\n",
    "        \n",
    "        c = clusterCreation(difflist, 8, obsPD, obs,obsMobility[0], action)\n",
    "#                 print(c)\n",
    "        gone = []\n",
    "        for h in c :\n",
    "            if h not in clusterDic:\n",
    "                clusterDic[h] = 0\n",
    "                record[h] = 0\n",
    "                        \n",
    "        for h in clusterDic:\n",
    "            if h in ls:\n",
    "                clusterDic[h] +=1     \n",
    "            else:\n",
    "                if h in record:\n",
    "                    gone.append(clusterDic[h])\n",
    "                    del record[h]\n",
    "        traci.simulationStep()\n",
    "        if len(gone) > 0:\n",
    "            clusterChange = True\n",
    "#             print(np.mean(gone))\n",
    "            reward = np.mean(gone) - 48.8\n",
    "            for x in obs:\n",
    "                obsOut.append(x[1][0])\n",
    "                obsOut.append(x[1][1])\n",
    "                obsOut.append(x[2])\n",
    "                obsOut.append(x[3])\n",
    "            if len(obsOut) < 150:\n",
    "                for x in range(0, 150 - len(obsOut)):\n",
    "                    obsOut.append(0)\n",
    "                          \n",
    "            break\n",
    "            \n",
    "    return obsOut,reward,done,obsMobility[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "MEMORY_SIZE = int(1e6)\n",
    "MEMORY_WARMUP_SIZE = 1e4\n",
    "BATCH_SIZE = 128\n",
    "REWARD_SCALE = 0.1\n",
    "ENV_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachEpisodeClusterLife = [] \n",
    "\n",
    "\n",
    "def run_train_episode(agent, rpm):\n",
    "    \n",
    "    traci.start([\"sumo\", \"-c\", \"highway.sumocfg\"])\n",
    "    obs,reward, done, ob = highway([0.8,0.1,0.1])\n",
    "    ClusterLife = []\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "#         print(batch_obs)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "\n",
    "        action = np.clip(np.random.normal(action, 1.0), 0, 1.0)\n",
    "\n",
    "        next_obs, reward, done, ob =  highway(action)\n",
    "        ClusterLife.append(reward)\n",
    "\n",
    "#         print(next_obs)\n",
    "\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        # print(\"rpm is \",rpm.sample_batch(\n",
    "        #         BATCH_SIZE))\n",
    "\n",
    "        # print(info)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, batch_terminal = rpm.sample_batch(\n",
    "                BATCH_SIZE)\n",
    "            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "                        batch_terminal)\n",
    "           \n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    eachEpisodeClusterLife.append(ClusterLife)\n",
    "#     print(total_reward)\n",
    "    traci.close()\n",
    "    print(\"close\")\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluate_episode(agent):\n",
    "        \n",
    "    traci.start([\"sumo\", \"-c\", \"highway.sumocfg\"])\n",
    "    obs,reward, done, ob = highway([0.8,0.1,0.1])\n",
    "    total_reward = 0\n",
    "    ClusterLife = []\n",
    "    while True:\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "#         print(action)\n",
    "\n",
    "        next_obs, reward, done,ob = highway(action)\n",
    "        ClusterLife.append(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    traci.close()\n",
    "    eachEpisodeClusterLife.append(ClusterLife)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parl.utils import check_version_for_fluid  # requires parl >= 1.4.1\n",
    "check_version_for_fluid()\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import parl\n",
    "\n",
    "from parl.utils import logger, ReplayMemory\n",
    "from parl.env.continuous_wrappers import ActionMappingWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 150\n",
    "act_dim = 3\n",
    "\n",
    "rewardCollected = []\n",
    "    \n",
    "model = MujocoModel(act_dim)\n",
    "algorithm = parl.algorithms.DDPG(\n",
    "model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "\n",
    "agent = MujocoAgent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "rpm = ReplayMemory(MEMORY_SIZE, obs_dim, act_dim)\n",
    "\n",
    "while rpm.size() < MEMORY_WARMUP_SIZE:\n",
    "    print(rpm.size())\n",
    "    clusterDic = {}\n",
    "    performance = {}\n",
    "    record = {}\n",
    "    run_train_episode(agent, rpm)\n",
    "\n",
    "print('xxxx')    \n",
    "episode = 0\n",
    "RewardCollect = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode < 1000 :\n",
    "    \n",
    "    for i in range(10):\n",
    "        clusterDic = {}\n",
    "        performance = {}\n",
    "        record = {}\n",
    "        train_reward = run_train_episode(agent, rpm)\n",
    "        rewardCollected.append(train_reward)\n",
    "        episode += 1\n",
    "        logger.info('Episode: {} Reward: {}'.format(episode, train_reward))\n",
    "        \n",
    "    clusterDic = {}\n",
    "    performance = {}\n",
    "    record = {}\n",
    "    evaluate_reward = run_evaluate_episode(agent)\n",
    "    rewardCollected.append(evaluate_reward)\n",
    "    logger.info('Episode {}, Evaluate reward: {}'.format(episode, evaluate_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "total = 0\n",
    "averageReward = []\n",
    "for x in rewardCollected1:\n",
    "    total += x\n",
    "    \n",
    "    averageReward.append(total / step)\n",
    "    step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(averageReward)), averageReward, color='green', label='Average Reward')\n",
    "\n",
    "plt.legend() # 显示图例\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.savefig(\"AveragerewardCollectedFinal.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterDic = {}\n",
    "performance = {}\n",
    "record = {}\n",
    "ClusterLifeFinalModel = []\n",
    "traci.start([\"sumo\", \"-c\", \"highway.sumocfg\"])\n",
    "obs,reward, done,ob = highway([0.8,0.1,0.1])\n",
    "total_reward = 0\n",
    "ClusterLife = []\n",
    "\n",
    "SpeedLife = {}\n",
    "AccLife = {}\n",
    "\n",
    "\n",
    "while True:\n",
    "    batch_obs = np.expand_dims(obs, axis=0)\n",
    "    action = agent.predict(batch_obs.astype('float32'))\n",
    "\n",
    "#     newac = []\n",
    "#     for i in action:\n",
    "#         newac.append(-i)\n",
    "#     action = np.clip(np.random.normal(action, 1.0), 0, 1.0)\n",
    "#     next_obs, reward, done,ob = highway(action)\n",
    "    next_obs, reward, done,ob =  highway([0.8,-2,0.1])\n",
    "    speed = round(ob[0], 1)\n",
    "    acc = round(ob[1],1)\n",
    "    if speed in SpeedLife:\n",
    "        SpeedLife[speed].append((reward + 48.8)/2)\n",
    "    else:\n",
    "        SpeedLife[speed] = [(reward + 48.8)/2]\n",
    "    print(SpeedLife[speed])\n",
    "    \n",
    "   \n",
    "   \n",
    "    if acc in AccLife:\n",
    "        AccLife[acc].append((reward + 48.8))\n",
    "    else:\n",
    "        AccLife[acc] = [reward + 48.8]\n",
    "   \n",
    "        \n",
    "    \n",
    "    \n",
    "    ClusterLifeFinalModel.append((reward + 48.8)/2)\n",
    "    obs = next_obs\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "traci.close()\n",
    "  \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterDic = {}\n",
    "performance = {}\n",
    "record = {}\n",
    "ClusterLifeFinalModelRL = []\n",
    "traci.start([\"sumo\", \"-c\", \"highway.sumocfg\"])\n",
    "obs,reward, done,ob = highway([0.8,0.1,0.1])\n",
    "total_reward = 0\n",
    "ClusterLife = []\n",
    "\n",
    "SpeedLifeRL = {}\n",
    "AccLifeRL = {}\n",
    "\n",
    "\n",
    "while True:\n",
    "    batch_obs = np.expand_dims(obs, axis=0)\n",
    "    action = agent.predict(batch_obs.astype('float32'))\n",
    "\n",
    "#     newac = []\n",
    "#     for i in action:\n",
    "#         newac.append(-i)\n",
    "#     action = np.clip(np.random.normal(action, 1.0), 0, 1.0)\n",
    "    next_obs, reward, done,ob = highway(action)\n",
    "#     next_obs, reward, done,ob = highway(action) highway([0.8,0.1,0.1])\n",
    "    speed = round(ob[0], 1)\n",
    "    acc = round(ob[1],1)\n",
    "    if speed in SpeedLifeRL:\n",
    "        SpeedLifeRL[speed].append((reward + 48.8)/2)\n",
    "    else:\n",
    "        SpeedLifeRL[speed] = [(reward + 48.8)/2]\n",
    "\n",
    "    \n",
    "   \n",
    "   \n",
    "    if acc in AccLifeRL:\n",
    "        AccLifeRL[acc].append((reward + 48.8))\n",
    "    else:\n",
    "        AccLifeRL[acc] = [reward + 48.8]\n",
    "   \n",
    "        \n",
    "    \n",
    "    \n",
    "    ClusterLifeFinalModelRL.append((reward + 48.8)/2)\n",
    "    obs = next_obs\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "traci.close()\n",
    "  \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRLAverage = []\n",
    "step = 1\n",
    "tt = 0\n",
    "totalLifeRL = 0\n",
    "for x in CRL:\n",
    "    totalLifeRL  += x\n",
    "    tt+= x\n",
    "    CRLAverage.append(tt/step)\n",
    "    step +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAverage = []\n",
    "step = 1\n",
    "tt = 0\n",
    "totalLife = 0\n",
    "for x in CWRL:\n",
    "    totalLife += x\n",
    "    tt+= x   \n",
    "    CAverage.append(tt/step)\n",
    "    step +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(CAverage)), CAverage, color='green', label='Fixed Weights')\n",
    "plt.plot(range(0,len(CRLAverage)), CRLAverage, color='red',label='RL Weights')\n",
    "\n",
    "plt.legend() # 显示图例\n",
    "\n",
    "plt.xlabel('Cluster Change Time')\n",
    "plt.ylabel('Average Cluster Life Time')\n",
    "plt.savefig(\"xxx1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forSpeedLifeDrowX = []\n",
    "forSpeedLifeDrowY = []\n",
    "for x in sorted(SpeedLife.keys()):\n",
    "    forSpeedLifeDrowX.append(float(x))\n",
    "    forSpeedLifeDrowY.append(float(np.mean(SpeedLife[x])))\n",
    "\n",
    "print(forSpeedLifeDrowX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forSpeedLifeDrowXRL = []\n",
    "forSpeedLifeDrowYRL = []\n",
    "for x in sorted(SpeedLifeRL.keys()):\n",
    "    forSpeedLifeDrowXRL.append(float(x))\n",
    "    forSpeedLifeDrowYRL.append(float(np.mean(SpeedLifeRL[x])))\n",
    "\n",
    "print(forSpeedLifeDrowXRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forSpeedLifeDrowX, forSpeedLifeDrowY, color='green', label='Fixed Weights')\n",
    "plt.plot(forSpeedLifeDrowXRL, forSpeedLifeDrowYRL, color='red',label='RL Weights')\n",
    "\n",
    "plt.legend() # 显示图例\n",
    "\n",
    "plt.xlabel('Average Speed')\n",
    "plt.ylabel('Cluster Life Time')\n",
    "plt.savefig(\"speed2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
